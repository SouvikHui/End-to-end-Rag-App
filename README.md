# ğŸ§  Online Article Analyzer (RAG-based)
### A lightweight full-stack [FastAPI (backend) + Streamlit (frontend)] Question-Answering (QA) system that allows users to input up to 5 article URLs, YouTube links, or attach PDF, DOCX, TXT, or audio files. For YouTube links and audio files, transcription is handled using the `whisper-large-v3-turbo` model via the Groq API. The fetched content is embedded using Nomic Embeddings, and context-aware answers are generated via the Groq LLM (`LLaMA-3.3-70B (Versatile)`). The app uses FAISS for fast vector search and LangChain to orchestrate the Retrieval-Augmented Generation (RAG) pipeline.

## âœ¨ Features

- ğŸ”— Accepts multiple **article URLs** (1 to 5)
- â–¶ï¸ Accepts **YouTube video links** for QA (audio transcribed using `whisper-large-v3-turbo`)
- ğŸ”Š Supports **audio file uploads** (e.g., .mp3, .wav) for QA
- ğŸ“„ Extracts and chunks content (text, audio transcripts)
- ğŸ§  Embeds using **Nomic Embeddings**
- ğŸ’¬ Ask context-specific questions
- âš¡ï¸ Answers generated by **Groq LLaMA-3.3-70B (Versatile)**
- ğŸ—‚ï¸ Uses **FAISS** for local vector storage
- ğŸš€ Built using **FastAPI (backend)** and **Streamlit (frontend)**


| Layer       | Technology                          |
|-------------|-------------------------------------|
| Frontend    | Streamlit UI                        |
| Backend     | FastAPI + LangChain                 |
| Embeddings  | Nomic Embeddings                    |
| Vector DB   | FAISS (local)                       |
|Transcription| Groq API (whisper-large-v3-turbo)   |
| LLM         | Groq API (LLaMA-3.3-70B (Versatile))|
| Loader      | Pypdf, BeautifulSoup, yt_dlp        |

## ğŸ“ Project Structure
<details>
<summary>ğŸ“ Click to expand project structure</summary>

```
project-root/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py # FastAPI server
â”‚   â”œâ”€â”€ models.py # Pydantic schemas
â”‚   â”œâ”€â”€ fetcher.py # URL & attached file (.pdf,.docx,.txt) loader
|   â”œâ”€â”€ yt_audio_fetcher.py # YT link & attched audio handling & transcription 
â”‚   â”œâ”€â”€ embed_data.py # Embedding logic
â”‚   â”œâ”€â”€ rag_qa.py # RAG QA pipeline
â”‚   â””â”€â”€ faiss_store/ # Saved FAISS index (Auto-created on app run)
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py # Streamlit frontend
â”‚
â”œâ”€â”€ .env # Your API keys (GROQ_API_KEY, NOMIC_API_KEY)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
</details>

## âš™ï¸ Setup Instructions
1. Clone the Repository:
  ```git clone https://github.com/SouvikHui/End-to-end-Rag-App.git```
  ```cd End-to-end-Rag-App```

2. Create and Activate Environment:
  ```conda create -n ragenv python=3.11.5```
  ```conda activate ragenv```

3. Install Dependencies:
```pip install -r requirements.txt```
    Include dependencies like: fastapi, uvicorn, streamlit, pypdf, yt-dlp, langchain, langchain_groq, langchain_community, python-dotenv, requests, etc.
4. Set Up Environment Variables
**Create a .env file in the root directory:
  GROQ_API_KEY=your_groq_api_key
  NOMIC_API_KEY=your_nomic_api_key

## â–¶ï¸ Running the App
1. Start Backend (FastAPI):
```uvicorn backend.main:app --reload```
By default, the backend runs at: http://localhost:8000
otherwise, use ```uvicorn backend.main:app --reload --host 127.0.0.1 --port 8000```
2. Start Frontend (Streamlit):
```streamlit run frontend/app.py```

## ğŸ§ª Example Usage
1. Paste 1â€“5 article URLs.
2. Or upload a file from local machine (PDF/DOCX/TXT)
3. Or input a YouTube video link (will transcribe audio)
4. Or upload an audio file (MP3/WAV)
5. Click â€œProcess Articlesâ€.
6. Ask questions about the content.
7. Get answers retrieved from the embedded article context.

## ğŸ“Œ Coming Soon
- ğŸ“Š .xls/.xlsx file support for tabular QA
- ğŸ§¾ Multiple attachment file support (multi-document QA)
- ğŸ—£ï¸ Speaker diarization for meetings
- ğŸ§  Multi-agent LLM QA chains
